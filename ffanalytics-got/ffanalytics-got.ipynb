{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pickling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_var(var,var_name):\n",
    "    dump_path=r'.'\n",
    "    dump_var=dump_path+\"\\\\\"+var_name+\".pickle\"\n",
    "    var_file=open(dump_var,\"wb\")\n",
    "    pk.dump(var,var_file)\n",
    "    var_file.close()\n",
    "    \n",
    "def load_var(var_name):\n",
    "    load_path=r'.'\n",
    "    load_var=load_path+\"\\\\\"+var_name+\".pickle\"\n",
    "    with open(load_var,\"rb\") as var_file:\n",
    "        print(load_var)\n",
    "        content=pk.load(var_file)\n",
    "    var_file.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = pd.read_csv(r'story.csv', header=None, error_bad_lines=False)\n",
    "user_favorite_author = pd.read_csv(r'author_favorites.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['id','fandom_id','user_id','rating_id','language_id',\n",
    "         'ff_story_id','title','chapters','words',\n",
    "         'reviews','favorites','followers','date_published',\n",
    "         'date_updated','is_complete']\n",
    "story.columns=columns\n",
    "story.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aggregated GOT Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ot=story[story.fandom_id == 8477][['user_id','id','reviews','chapters','favorites','followers','words']].groupby('user_id').agg({\n",
    "    'id':['count'],\n",
    "    'reviews':['sum'],\n",
    "    'chapters':['sum'],\n",
    "    'favorites':['sum','mean'],\n",
    "    'followers':['sum','mean'],\n",
    "    'words':['sum']\n",
    "})\n",
    "game_ot.columns = [\"_\".join(x) for x in game_ot.columns.ravel()]\n",
    "GOT=game_ot.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_GOT=sorted(np.array(GOT['user_id']))\n",
    "user_authors=np.array(user_favorite_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extract list of users and favourited authors from user_author table (very time consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_author_got=np.array( [[users[0],users[1]] for users in user_authors if users[0] in user_id_GOT and users[1] in user_id_GOT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Unique number of users and authors favourited by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Users: \"+str(len(np.unique(user_author_got[:,0]))))\n",
    "print(\"Authors favorited by users: \"+str(len(np.unique(user_author_got[:,1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_list=np.array(user_author_got)\n",
    "\n",
    "s1=set(kv_list[:,0])\n",
    "s2=set(kv_list[:,1])\n",
    "s=s1.union(s2)\n",
    "AuthorList=list(s)\n",
    "AuthorList.sort()\n",
    "AuthorDict={}\n",
    "for i in range(len(AuthorList)):\n",
    "    AuthorDict[AuthorList[i]]=i\n",
    "\n",
    "NormKVList=np.zeros(shape=(len(kv_list),2))\n",
    "for i in range(len(kv_list)):\n",
    "    NormKVList[i][0]=AuthorDict[kv_list[i][0]]\n",
    "    NormKVList[i][1]=AuthorDict[kv_list[i][1]]\n",
    "\n",
    "# Define a zero Matrix with length as number of unique journals\n",
    "len_NormKV_list=len(NormKVList)\n",
    "AdjMat=np.zeros(shape=(len(AuthorList),len(AuthorList)))\n",
    "\n",
    "for i in range(len_NormKV_list):\n",
    "    AdjMat[int(NormKVList[i][1]),int(NormKVList[i][0])]=1\n",
    "\n",
    "# Get column sums\n",
    "ColSum=AdjMat.sum(axis=0)\n",
    "ColSum1=np.array(ColSum)\n",
    "\n",
    "# Set the column sums with value 0 to -1 (to avoid 0/0 and X/0 divisions)\n",
    "ColSum1[ColSum1==0]=-1\n",
    "\n",
    "# Normaliza the matrix\n",
    "AdjMat_norm=np.true_divide(AdjMat, ColSum1)\n",
    "\n",
    "# Dangling Nodes\n",
    "Dang=np.array(ColSum1)\n",
    "Dang[Dang>0]=0\n",
    "Dang[Dang==-1]=1\n",
    "\n",
    "# Teleoport Vector\n",
    "TeleportVector=np.full(len(AuthorList),(1/len(AuthorList)))\n",
    "InitialStartVector=np.array(TeleportVector)\n",
    "\n",
    "# PageRank Parameters\n",
    "alpha=0.85\n",
    "e=0.0001\n",
    "H=np.array(AdjMat_norm)\n",
    "pi_0=np.array(InitialStartVector)\n",
    "d=np.array(Dang)\n",
    "pi_k=np.array(pi_0)\n",
    "pi_k1=np.empty(len(pi_0))\n",
    "run_flag=True\n",
    "a=np.array(TeleportVector)\n",
    "\n",
    "countIterations=0\n",
    "run_flag=True\n",
    "def l1(pi_1,pi_2):\n",
    "    return np.round(np.linalg.norm((pi_1-pi_2),1),5)\n",
    "\n",
    "    ### l1 output rounded off to 5 digits as e=0.00001\n",
    "\n",
    "while(run_flag):\n",
    "    countIterations+=1\n",
    "    pi_k1=np.array(alpha*(H.dot(pi_k))+(alpha*(d.dot(pi_k))+(1-alpha))*a) #Pagerank equation\n",
    "    if l1(pi_k,pi_k1)<e:\n",
    "        run_flag=False\n",
    "    pi_k=np.array(pi_k1)\n",
    "\n",
    "\n",
    "EF=100*np.true_divide(H.dot(pi_k),np.sum(H.dot(pi_k)))\n",
    "\n",
    "# Generate Index\n",
    "EF_index=np.arange(0,len(AuthorList))\n",
    "\n",
    "# Concat two arrays\n",
    "EF_final=np.vstack((EF, EF_index))\n",
    "\n",
    "EF_finalT=EF_final.T\n",
    "\n",
    "# Sort the journals and get top 20\n",
    "sortedEF_finalT=EF_finalT[EF_finalT[:,0].argsort()[::-1]][0:20]\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in AuthorDict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "\n",
    "# Get Authors from Dictionary\n",
    "for i in range(len(sortedEF_finalT)):\n",
    "    print(\"Score: \"+str(sortedEF_finalT[i][0])+\" Author:\"+str(get_key(sortedEF_finalT[i][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks=np.zeros(shape=(len(EF_finalT),2))\n",
    "\n",
    "for i in range(len(EF_finalT)):\n",
    "    pageranks[i][0]=get_key(EF_finalT[i][1])\n",
    "    pageranks[i][1]=EF_finalT[i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 If PageRanks are computed earlier, then load the file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageranks=pd.read_csv('pagerank.csv')\n",
    "pageranks=pageranks.drop(columns=['Unnamed: 0'])\n",
    "pgs=pd.DataFrame(pageranks,columns=['user_id','score']).astype({'user_id':np.int64,'score':float})\n",
    "#merged_scores=GOT.merge(pgs,how='left',on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_scores=GOT.merge(pagerank,how='left',on='user_id')\n",
    "merged_scores[(merged_scores['scores']!=0) & (merged_scores['scores']!=np.nan)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Plot Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_df=pd.read_csv(r'C:\\Users\\ajink\\Documents\\drg\\data\\pg.csv')\n",
    "pg_df.columns=['id','rank']\n",
    "pg_df1=pg_df[pg_df['rank']!=0]\n",
    "fig, ax=plt.subplots(1,1,figsize=(10,8))\n",
    "ax.set_ylabel(\"Frequency (log)\")\n",
    "ax.set_xlabel(\"Pageranks\")\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, 0.5)\n",
    "ax.hist(pg_df1['rank'],bins=100,color='Green')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Filter the records based on nulls and zeros scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT_filtered=merged_scores[(merged_scores['score']!=0) & (merged_scores['score'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT_filtered['story_chapter_product']=GOT_filtered['id_count']*GOT_filtered['chapters_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list=['reviews_sum','favorites_sum','words_sum','story_chapter_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_tr=np.array(GOT_filtered_1['score'])\n",
    "results=[]\n",
    "slope=[]\n",
    "intercept=[]\n",
    "x_tr=[]\n",
    "r2=[]\n",
    "\n",
    "for index, x_data in enumerate(feature_list):\n",
    "    x_tr.append(np.array(GOT_filtered_1[[x_data]]))\n",
    "\n",
    "for index, x_data in enumerate(feature_list):\n",
    "    reg = LinearRegression().fit(x_tr[index], y_tr)\n",
    "    r2.append(reg.score(x_tr[index], y_tr))\n",
    "    slope.append(reg.coef_)\n",
    "    intercept.append(reg.intercept_)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 20))\n",
    "for index, x_data in enumerate(feature_list):\n",
    "    axs[int(index/2)][index%2].scatter(x_tr[index],y_tr,c='c',alpha=0.33)\n",
    "    y = slope[index]*x_tr[index] + intercept[index]\n",
    "    axs[int(index/2)][index%2].plot(x_tr[index], y, 'm-')\n",
    "    axs[int(index/2)][index%2].set_title(\" PageRank ~ \"+str(x_data)+\", R-Squared=\"+str(round(r2[index],5)))\n",
    "    axs[int(index/2)][index%2].set_xlabel(x_data)\n",
    "    axs[int(index/2)][index%2].set_ylabel('Scores')\n",
    "plt.show()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
