{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Embedding, Input, TimeDistributed, Activation\n",
    "from tensorflow.keras.layers import MaxPooling1D, SpatialDropout1D, Dropout, Concatenate, Flatten, RepeatVector, Permute\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Multiply, Lambda\n",
    "from tensorflow.keras.backend import sum as Ksum\n",
    "\n",
    "from tensorflow.keras.models import load_model, model_from_json\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_df=pd.read_csv('labeled_data.txt',sep='|')\n",
    "labeled_data_df[['Addresses']]=pd.DataFrame(labeled_data_df['Addresses'].apply(lambda t:ast.literal_eval(t)))\n",
    "labeled_data_df[['Notice']]=pd.DataFrame(labeled_data_df['Notice'].apply(lambda t:t.replace('\\n',' ')))\n",
    "labeled_data_df[['Notice']]=pd.DataFrame(labeled_data_df['Notice'].apply(lambda t:' '.join(t.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnest(df, tile, explode):\n",
    "    vals = df[explode].sum(1)\n",
    "    rs = [len(r) for r in vals]\n",
    "    a = np.repeat(df[tile].values, rs, axis=0)\n",
    "    b = np.concatenate(vals.values)\n",
    "    d = np.column_stack((a, b))\n",
    "    return pd.DataFrame(d, columns = tile +  ['_'.join(explode)])\n",
    "def join_textseq(seq_list,start_char='<',end_char='>'):\n",
    "    new_seq_list=[]\n",
    "    for seq in seq_list:\n",
    "        new_seq_list.append('<'+seq+'>')\n",
    "    return ''.join(new_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=unnest(labeled_data_df, ['Notice'], ['Addresses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Notice</th>\n",
       "      <th>Addresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Eviction Notice, also known as a Notice to ...</td>\n",
       "      <td>PSC 8763, Box 5203 APO AE 49080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This record may be useful in case of future le...</td>\n",
       "      <td>311 Rosario Haven Suite 194 Jessicamouth, HI 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This record may be useful in case of future le...</td>\n",
       "      <td>81965 Harris Fall Suite 885 Hallhaven, WV 96622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>At the end, you receive it in Word and PDF for...</td>\n",
       "      <td>81888 Hart Turnpike Apt. 641 West Jesse, MT 11649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At the end, you receive it in Word and PDF for...</td>\n",
       "      <td>5770 Martinez Mountain Apt. 482 Matthewland, K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Notice  \\\n",
       "0  An Eviction Notice, also known as a Notice to ...   \n",
       "1  This record may be useful in case of future le...   \n",
       "2  This record may be useful in case of future le...   \n",
       "3  At the end, you receive it in Word and PDF for...   \n",
       "4  At the end, you receive it in Word and PDF for...   \n",
       "\n",
       "                                           Addresses  \n",
       "0                    PSC 8763, Box 5203 APO AE 49080  \n",
       "1  311 Rosario Haven Suite 194 Jessicamouth, HI 1...  \n",
       "2    81965 Harris Fall Suite 885 Hallhaven, WV 96622  \n",
       "3  81888 Hart Turnpike Apt. 641 West Jesse, MT 11649  \n",
       "4  5770 Martinez Mountain Apt. 482 Matthewland, K...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_m2=labeled_data_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_m2['Addresses']=labeled_data_df['Addresses'].apply(lambda l:list(l)).apply(lambda l:join_textseq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Notice       to hany renter bradford address subject evicti...\n",
       "Addresses                                                   ()\n",
       "Name: 1413, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data_df.iloc[1413]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Feature Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add hash and dash sign as well\n",
    "features='1234567890abcdefghijklmnopqrstuvwxyz <>'\n",
    "feature_dict=defaultdict(int)\n",
    "count=0\n",
    "for f in features:\n",
    "    count+=1\n",
    "    feature_dict[f] += count # increment element's value by 1\n",
    "\n",
    "inv_feature_dict = {v: k for k, v in feature_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Encode/Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_seq(text, feature_dict=feature_dict):\n",
    "    code=[]\n",
    "    text=text.lower()\n",
    "    text='<'+text+'>'\n",
    "    for charac in text:\n",
    "        code.append(feature_dict[charac])\n",
    "    return code\n",
    "\n",
    "def decode_label_seq(seq, mapping=inv_feature_dict):\n",
    "    seq_d=[]\n",
    "    for num in seq:\n",
    "        n=int(np.round(num))\n",
    "        if n!= 0:\n",
    "            seq_d.append(mapping[n])\n",
    "        else:\n",
    "            seq_d.append('_')\n",
    "    return ''.join(seq_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(seq_array):\n",
    "    MAX_LABEL_SEQ_LEN=0\n",
    "    for seq in seq_array:\n",
    "        if len(seq) > MAX_LABEL_SEQ_LEN:\n",
    "            MAX_LABEL_SEQ_LEN=len(seq) \n",
    "    return MAX_LABEL_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_vector(np_arr, onehot_length=800):\n",
    "    np_arr=np.array(np_arr)\n",
    "    #flat_arr=[item for seq in np_arr for item in seq]\n",
    "    if onehot_length < max(np_arr)+1:\n",
    "        onehot_length=max(np_arr)+1\n",
    "    encoded_matrix=np.zeros((np_arr.shape[0],onehot_length))\n",
    "    for i in range(np_arr.shape[0]):\n",
    "            encoded_matrix[i,np_arr[i]]\n",
    "    return encoded_matrix\n",
    "def onehot_encode_matrix(np_arr):\n",
    "    flat_arr=[item for seq in np_arr for item in seq]\n",
    "    len_enc=max(flat_arr)+1\n",
    "    encoded_matrix=np.zeros((np_arr.shape[0],np_arr.shape[1],len_enc))\n",
    "    for i in range(np_arr.shape[0]):\n",
    "        for j in range(np_arr.shape[1]):\n",
    "            encoded_matrix[i,j,np_arr[i,j]]=1\n",
    "    return encoded_matrix   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Feature Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- M1 - Modelling approach: Spearate Out multiple addresses \n",
    "- M2 - Modelling approach: Do not spearate Out multiple addresses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_encoder_seq=data_df['Notice'].apply(lambda l:encode_label_seq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 8040\n"
     ]
    }
   ],
   "source": [
    "print(\"Max sequence length:\",get_max_length(inp_encoder_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pad_sequences(inp_encoder_seq, 10000, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Decoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_seq=data_df['Addresses'].apply(lambda l:encode_label_seq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max decoder sequence length: 67\n"
     ]
    }
   ],
   "source": [
    "print(\"Max decoder sequence length:\",get_max_length(decoder_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in=pad_sequences([s[:-1] for s in decoder_seq],100,padding='post')\n",
    "D_ou=onehot_encode_matrix(pad_sequences([s[1:] for s in decoder_seq],100,padding='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_encoder_seq_m2=data_df_m2['Notice'].apply(lambda l:encode_label_seq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m2=pad_sequences(inp_encoder_seq_m2, 10000, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Decoder Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_seq_m2=data_df_m2['Addresses'].apply(lambda l:encode_label_seq(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max decoder sequence length: 175\n"
     ]
    }
   ],
   "source": [
    "print(\"Max decoder sequence length:\",get_max_length(decoder_seq_m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in_m2=pad_sequences([s[:-1] for s in decoder_seq_m2],200,padding='post')\n",
    "D_ou_m2=onehot_encode_matrix(pad_sequences([s[1:] for s in decoder_seq_m2],200,padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,name_suffix=''):\n",
    "    model_json = model.to_json()\n",
    "    model_name='model'+name_suffix\n",
    "    json_filename=model_name+'.json'\n",
    "    hdf5_filename=model_name+'.h5'\n",
    "    with open(json_filename, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(hdf5_filename)\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_model(name_suffix):\n",
    "    model_name='model'+name_suffix\n",
    "    json_filename=model_name+'.json'\n",
    "    hdf5_filename=model_name+'.h5'\n",
    "    json_file = open(json_filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(hdf5_filename)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens=len(feature_dict)+1 # +1 to default character\n",
    "num_decoder_tokens=len(feature_dict)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding size rule based on https://forums.fast.ai/t/embedding-layer-size-rule/50691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(600, round(1.6 * num_encoder_tokens ** .56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Encoder tokens 85\n",
      "# Decoder tokens 85\n",
      "# embdedding dims 20\n",
      "# Encoder Input (2514, 10000)\n",
      "# Decoder Input (2514, 100)\n",
      "# Decoder Input (2514, 100, 40)\n"
     ]
    }
   ],
   "source": [
    "print('# Encoder tokens',num_encoder_tokens)\n",
    "print('# Decoder tokens',num_decoder_tokens)\n",
    "print('# embdedding dims',latent_dim)\n",
    "print('# Encoder Input',X.shape)\n",
    "print('# Decoder Input',D_in.shape)\n",
    "print('# Decoder Input',D_ou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "x, _,_ = LSTM(latent_dim, return_sequences=True, return_state=True)(x, initial_state=encoder_states)\n",
    "#x = Lambda(lambda xin: Ksum(xin, axis=1))(x)\n",
    "decoder_outputs = Dense(40, activation='softmax')(x)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=120\n",
    "batch_size=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 20)     1700        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 20)     1700        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 20), (None,  3280        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 20), ( 3280        embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 40)     840         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 10,800\n",
      "Trainable params: 10,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit([X, D_in], D_ou,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model,'m1_')\n",
    "with open('m1_history_dict', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Encoder Input (1443, 10000)\n",
      "# Decoder Input (1443, 200)\n",
      "# Decoder Input (1443, 200, 40)\n"
     ]
    }
   ],
   "source": [
    "print('# Encoder Input',X_m2.shape)\n",
    "print('# Decoder Input',D_in_m2.shape)\n",
    "print('# Decoder Input',D_ou_m2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "x, _,_ = LSTM(latent_dim, return_sequences=True, return_state=True)(x, initial_state=encoder_states)\n",
    "#x = Lambda(lambda xin: Ksum(xin, axis=1))(x)\n",
    "decoder_outputs = Dense(40, activation='softmax')(x)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model_m2 = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile & run training\n",
    "model_m2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=120\n",
    "batch_size=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 20)     1700        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 20)     1700        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 20), (None,  3280        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 20), ( 3280        embedding_3[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 40)     840         lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 10,800\n",
      "Trainable params: 10,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model_m2.fit([X_m2, D_in_m2], D_ou_m2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model_m2,'m2_')\n",
    "with open('m1_history_dict', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=reload_model('m1_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Validate over Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_output(seq_num, model, train_df=labeled_data_df):\n",
    "    tr=train_df.iloc[1]['Notice']\n",
    "    tl=train_df.iloc[1]['Addresses'][0]\n",
    "    tr_in=np.array(encode_label_seq(tr))\n",
    "    tr_in2=np.zeros(99)\n",
    "    tr_in2[0]=38\n",
    "    #tr_in2=np.array(encode_label_seq('311 Rosario Haven Suite 194 Jessicamouth, HI 19398'))\n",
    "    print('Train Label:',tl)\n",
    "    tp_ou=m1.predict([[tr_in],[tr_in2]])\n",
    "    tp=argmax(tp_ou,axis=2)\n",
    "    print('Predicted Label\\n',decode_label_seq(tp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label: 311 Rosario Haven Suite 194 Jessicamouth, HI 19398\n",
      "Predicted Label\n",
      " 83                                                                _________________________________\n"
     ]
    }
   ],
   "source": [
    "tr=labeled_data_df.iloc[1]['Notice']\n",
    "tl=labeled_data_df.iloc[1]['Addresses'][0]\n",
    "tr_in=np.array(encode_label_seq(tr))\n",
    "tr_in2=np.zeros(99)\n",
    "tr_in2[0]=38\n",
    "#tr_in2=np.array(encode_label_seq('311 Rosario Haven Suite 194 Jessicamouth, HI 19398'))\n",
    "print('Train Label:',tl)\n",
    "tp_ou=m1.predict([[tr_in],[tr_in2]])\n",
    "tp=argmax(tp_ou,axis=2)\n",
    "print('Predicted Label\\n',decode_label_seq(tp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def predict_feed(model, encoder_input, decoder_input)\n",
    "encoder_input=tr_in\n",
    "decoder_input=tr_in2\n",
    "\n",
    "for i in range(len(decoder_input)-1):\n",
    "    next_char=argmax(m1.predict([[encoder_input],[decoder_input]]),axis=2)[0][i+1]\n",
    "    decoder_input[i+1]=next_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<3 777 a pa 777 777________________________________________________________________________________'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_label_seq(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  8,  8, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax(m1.predict([[encoder_input],[decoder_input]]),axis=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax(m1.predict([[encoder_input],[decoder_input]]),axis=2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  3, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label: 311 Rosario Haven Suite 194 Jessicamouth, HI 19398\n",
      "Predicted Label\n",
      " 83                                                                _________________________________\n"
     ]
    }
   ],
   "source": [
    "compare_output(11,m1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
