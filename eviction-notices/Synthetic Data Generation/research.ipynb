{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach  \n",
    "1. Get some random texts from eviction notices (10-30)\n",
    "2. Augument these random texts to 10000 using text augmentation techniques\n",
    "3. Randomly generate fake addresses and insert them into specific and random positions in the 10,000 texts\n",
    "4. At the same time assign the generated fake addresses as a label to that text\n",
    "5. LSTM\n",
    "6. User Address parser to extract and label address entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission\n",
    "- Report\n",
    "- Code Documentation\n",
    "- Functions input and Output\n",
    "- Usage\n",
    "- Limiyations\n",
    "- size of input text, output text - pages, words, rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New IS  \n",
    "- Finshed trainng dat acreation  \n",
    "- Modelling part for next quarter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ajink\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import usaddress\n",
    "import pytesseract\n",
    "import cv2\n",
    "import glob\n",
    "import faker\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import spacy\n",
    "from py_thesaurus import Thesaurus\n",
    "from nltk.corpus import wordnet \n",
    "import en_core_web_sm\n",
    "import re\n",
    "from nlp_aug import *\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "#### Parse Addresses  \n",
    "For International addresses: 'https://github.com/openvenues/libpostal'  \n",
    "For USA Addresses: https://github.com/datamade/usaddress\n",
    "#### OCR\n",
    "https://pypi.org/project/pytesseract/\n",
    "#### Supporting Libraries\n",
    "cv2, image/PIL.image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Augmentation Techniques  \n",
    "- Synonym Replacement (SR): Randomly replace n words in the sentences with their synonyms.\n",
    "- Random Insertion (RI): Insert random synonyms of words in a sentence, this is done n times.\n",
    "- Random Swap (RS): Two words in the sentences are randomly swapped, this is repeated n-times.\n",
    "- Random Deletion (RD): Random removal for each word in the sentence with a probability p.  \n",
    "The formula used to determine the number of sentences augmented is:  \n",
    "N = Alpha * Length of the sentence.  \n",
    "Alpha is the “augmentation parameter”, higher the alpha-more aggressive the “EDA”.  (Easy Data Augmentation)\n",
    "  \n",
    "Functions: https://www.kaggle.com/init927/nlp-data-augmentation\n",
    "Paper: https://arxiv.org/pdf/1901.11196.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checker\n",
    "https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "1. Creation of Synthetic Data\n",
    "        1. Gather Text\n",
    "        2. Convert Text into Documents\n",
    "        3. Add placeholders to the Text in decided and random locations\n",
    "            - %%ADDRESS%%, %%NAME%%, %%DATE%%, %%EMAIL%%, %%PHONE%%\n",
    "        4. Augment the Text along without touching the placeholders\n",
    "        5. Utility to replace the placeholders with corresponding randomly generated texts\n",
    "        6. Create a labeled dataset of (Documents, Addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake=faker.Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['synthetic-data\\\\1575100985_kcaV3Ce9GL_eviction_notice.docx', 'synthetic-data\\\\1575101025_nJ8RX5bZVI_eviction_notice.docx', 'synthetic-data\\\\1575101089_qxkhJFBaBW_eviction_notice.docx', 'synthetic-data\\\\1575101124_Zjdn8Ghl04_eviction_notice.docx', 'synthetic-data\\\\extract.txt', 'synthetic-data\\\\image (1).jpg', 'synthetic-data\\\\image (2).jpg', 'synthetic-data\\\\image (3).jpg', 'synthetic-data\\\\sources.txt', 'synthetic-data\\\\text']\n",
      "[Apariment Community Name]\n",
      "[street Address}\n",
      "[city, State, Zip]\n",
      "[Phone number and email address]\n",
      "TenantName(s\n",
      "Tenant Street Address]\n",
      "Tenant Apartment Number}\n",
      "[City, State, Zip]\n",
      "Dear Mr.Ms.[Tenant LastName(s],\n",
      "Multiple warnings have been issuedregarding [state reason forissuing eviction\n",
      "Notice), Yourrental agreement clearly states in [state section of lease containing\n",
      "violated policy and exactly what was violated), Dueto thetallure onyourpartto\n",
      "Uupholdthe agreement of your rental the [Apartment Community Name] has no\n",
      "choice butto submitthis natice of eviction\n",
      "‘Youhave been givena total offamount oftime]to [state actionsto betaken to avoid\n",
      "eviction] Failure to comply wil result in legal action, up to and including physical\n",
      "removal ofall tenants from the apartment andthe property.\n",
      "lyouhaveany questions regardingthisissue, please contactthe rental office and\n",
      "ask for [Property Manager Name}\n",
      "Sincerely,\n",
      "[Property Manager Name]\n",
      "\n",
      "Property Manager\n"
     ]
    }
   ],
   "source": [
    "img_file_list=glob.glob(r\"synthetic-data\\*\")\n",
    "print(img_file_list)\n",
    "img = cv2.imread(r'synthetic-data\\image (3).jpg')\n",
    "print(pytesseract.image_to_string(img))\n",
    "# OR explicit beforehand converting\n",
    "#print(pytesseract.image_to_string(Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_extract(file_path=r\".\\synthetic-data\\extract.txt\"):\n",
    "    extract=open(file_path,'r',encoding='utf-8')\n",
    "    extracted=extract.read()\n",
    "    extract.close()\n",
    "    return extracted\n",
    "\n",
    "def fix_extract(extract, threshold=4):\n",
    "    sentences=nltk.sent_tokenize(extract)\n",
    "    filtered_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        if len(nltk.word_tokenize(sentence))>4:\n",
    "            filtered_sentences.append(sentence)\n",
    "    return \" \".join(filtered_sentences)\n",
    "        \n",
    "def chunk_text(large_chunk,num=10):\n",
    "    sentences=nltk.sent_tokenize(extracted)\n",
    "    size=len(sentences)\n",
    "    mini_chunks=[]\n",
    "    start=0\n",
    "    end=num\n",
    "    for i in range(size//num):\n",
    "        mini_chunks.append(\" \".join(sentences[start:end]))\n",
    "        start=start+num\n",
    "        end=end+num\n",
    "    return mini_chunks\n",
    "\n",
    "def read_samples(file_path=r\".\\synthetic-data\\text\\*\"):\n",
    "    file_list = glob.glob(file_path)\n",
    "    corpus = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path) as f_input:\n",
    "            corpus.append(f_input.read())\n",
    "    return corpus\n",
    "\n",
    "def insert_placeholder(string, index, placeholder):\n",
    "    return string[:index] + '\\n'+placeholder+'\\n' + string[index:]\n",
    "\n",
    "def randomly_assign_placeholder(mini_chunk,probabilities=[0.8,0.4,0.7], placeholder=r'%%ADDRESS%%'):\n",
    "    chunk_size=len(mini_chunk)\n",
    "    if np.random.uniform(0,1)<probabilities[0]:\n",
    "        index=random.randint(0,chunk_size//3)\n",
    "        index=index+mini_chunk[index:].find(' ')\n",
    "        mini_chunk=insert_placeholder(mini_chunk,index,placeholder)\n",
    "    chunk_size=len(mini_chunk)\n",
    "    if np.random.uniform(0,1)<probabilities[1]:\n",
    "        index=random.randint(chunk_size//3,2*chunk_size//3)\n",
    "        index=index+mini_chunk[index:].find(' ')\n",
    "        mini_chunk=insert_placeholder(mini_chunk,index,placeholder)\n",
    "    chunk_size=len(mini_chunk)\n",
    "    if np.random.uniform(0,1)<probabilities[0]:\n",
    "        index=random.randint(2*chunk_size//3,chunk_size)\n",
    "        index=index+mini_chunk[index:].rfind(' ')\n",
    "        mini_chunk=insert_placeholder(mini_chunk,index,placeholder)\n",
    "    return mini_chunk\n",
    "\n",
    "def randomly_assign_placeholder_to_list(mini_chunks,probabilities=[0.8,0.4,0.7],placeholder=r'%%ADDRESS%%'):\n",
    "    processed_chunks=[]\n",
    "    for chunk in mini_chunks:\n",
    "        new_chunk=randomly_assign_placeholder(chunk,probabilities,placeholder)\n",
    "        processed_chunks.append(new_chunk)\n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted=fix_extract(read_extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_chunks=chunk_text(extracted,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Eviction Notice, also known as a Notice to Quit, is a document sent by a Landlord to a Tenant to inform them of a violation or termination of the lease agreement and to start the process of removing a Tenant from the property. This process can be very technical and varies from state to state. However, this eviction process generally begins with the Landlord providing the Tenant with a written Eviction Notice. Acceptable reasons to evict a tenant range from issues such as nonpayment of rent, violation of the lease terms, engaging in illegal activities on the premises, ending a month to month lease, or a Tenant staying on the premises after the lease has been terminated. The Eviction Notice serves to make the Tenant aware that they have not complied with the terms of the lease or are otherwise subject to being evicted and gives the Tenant a deadline by which they must either correct the issue or make arrangements to leave the property. If the Tenant does not cure the violation or leave the property, the Landlord can proceed with further legal action to reclaim the property according to the process dictated by relevant state law. How to use this document\\n\\nUse this document to inform a Tenant that they have violated their lease or their lease agreement is otherwise being terminated prior to the Landlord filing an eviction action in court. This Notice includes important facts about the property and lease agreement, including the parties to the lease, address of the rental property, the date the Tenant entered into the lease agreement, a description of the reason why the Tenant is being evicted, and, if applicable, instructions about how the Tenant can cure the lease violation and the deadline by which they must take action to cure their violation and come in compliance with the lease. Once the Landlord has completed the Eviction Notice, it is generally accepted practice for them to sign and date it and make arrangements to provide the Tenant with the Notice, being sure to keep a copy of the Notice for their own files in case of future legal action. Generally, an Eviction Notice is sent by certified mail or delivered in person, so as to create a record that the letter was sent and received by the Tenant.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synalter_Noun_Verb(word,al,POS):\n",
    "    max_temp = -1\n",
    "    flag = 0\n",
    "    for i in a1:\n",
    "        try:\n",
    "            w1 = wordnet.synset(word+'.'+POS+'.01') \n",
    "            w2 = wordnet.synset(i+'.'+POS+'.01') # n denotes noun \n",
    "            if(max_temp<w1.wup_similarity(w2)):\n",
    "                max_temp=w1.wup_similarity(w2)\n",
    "                temp_name = i\n",
    "                flag =1\n",
    "        except:\n",
    "            f = 0\n",
    "            \n",
    "    if flag == 0:\n",
    "        max1 = -1.\n",
    "        nlp = en_core_web_sm.load()\n",
    "        for i in a1:\n",
    "            j=i.replace(' ', '')\n",
    "            tokens = nlp(u''+j)\n",
    "            token_main = nlp(u''+word_str)\n",
    "            for token1 in token_main:\n",
    "                if max1<float(token1.similarity(tokens)):\n",
    "                    max1 = token1.similarity(tokens)\n",
    "                    value = i\n",
    "        max1 = -1.\n",
    "        return value \n",
    "    else:\n",
    "        return temp_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonyms(chunks,percent=50):\n",
    "    output_chunks=[]\n",
    "    for chunk in chunks:\n",
    "        output_chunk = chunk\n",
    "        words = chunk.split()\n",
    "        counts = {}\n",
    "        for word in words:\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "        one_word = []\n",
    "        for key, value in counts.items():\n",
    "            if value == 1 and key.isalpha() and len(key)>2:\n",
    "                one_word.append(key)\n",
    "        noun = []\n",
    "        verb = []\n",
    "        nlp = en_core_web_sm.load()\n",
    "        doc = nlp(u''+' '.join(one_word))\n",
    "        for token in doc:\n",
    "            if  token.pos_ == 'VERB':\n",
    "                verb.append(token.text)\n",
    "            if  token.pos_ == 'NOUN':\n",
    "                noun.append(token.text)\n",
    "        all_main =verb + noun\n",
    "        len_all = len(noun)+len(verb)\n",
    "        final_value = int(len_all * percent /100)\n",
    "        #random.seed(4)\n",
    "        temp = random.sample(range(0, len_all), final_value)\n",
    "        for i in temp:\n",
    "            try:\n",
    "                word_str = all_main[i]\n",
    "                w = Word(word_str)\n",
    "                a1= list(w.synonyms())\n",
    "                if i<len(verb):\n",
    "                    change_word=synalter_Noun_Verb(word_str,a1,'v')\n",
    "                    try:\n",
    "                        search_word = re.search(r'\\b('+word_str+r')\\b', output_chunk)\n",
    "                        Loc = search_word.start()\n",
    "                        output_chunk = output_chunk[:int(Loc)] + change_word + output_chunk[int(Loc) + len(word_str):]\n",
    "                    except:\n",
    "                        f=0\n",
    "\n",
    "                else:\n",
    "                    change_word=synalter_Noun_Verb(word_str,a1,'n')\n",
    "                    try:\n",
    "                        search_word = re.search(r'\\b('+word_str+r')\\b', output_chunk)\n",
    "                        Loc = search_word.start()\n",
    "                        output_chunk = output_chunk[:int(Loc)] + change_word + output_chunk[int(Loc) + len(word_str):]\n",
    "                    except:\n",
    "                        f=0\n",
    "            except:\n",
    "                f=0\n",
    "        output_chunks.append(output_chunk)\n",
    "    return output_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Augment Functions built upon nlp_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_chunk(mini_chunk, probability=0.5):\n",
    "    sentences=nltk.sent_tokenize(mini_chunk)\n",
    "    new_sentences=[]\n",
    "    for sentence in sentences:      \n",
    "        if random.uniform(0,1)<=probability:\n",
    "            new_sentence=eda_4(sentences[0], num_aug=1)[0]+'.'\n",
    "            new_sentences.append(new_sentence)\n",
    "        else:\n",
    "            new_sentences.append(sentence)\n",
    "    return \" \".join(new_sentences)\n",
    "\n",
    "def augment_list_of_chunks(mini_chunks,probability=0.5, num_aug=100):\n",
    "    augmented_chunks=[]\n",
    "    num=0\n",
    "    for i in range(num_aug):\n",
    "        for chunk in mini_chunks:\n",
    "            try:\n",
    "                num=num+1\n",
    "                augmented_chunks.append(augment_chunk(chunk,probability)) \n",
    "            except:\n",
    "                print(\"Error in Chunk List: Chunk#\" +str(num-1))\n",
    "                return None\n",
    "    augmented_chunks.extend(mini_chunks)\n",
    "    return augmented_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment chunks derived from the text extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_chunks=augment_list_of_chunks(mini_chunks, num_aug=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Address placeholders to the augmented chunks derived from the text extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks=randomly_assign_placeholder_to_list(ag_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Samples and Augment Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_notices=read_samples()\n",
    "aug_sample_notices=augment_list_of_chunks(sample_notices, num_aug=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data=processed_chunks+aug_sample_notices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace %%ADDRESS%%  placeholders with random fake addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurences(source_string, substring):\n",
    "    return re.subn(substring, '', source_string)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/27589325/how-to-find-and-replace-nth-occurrence-of-word-in-a-sentence-using-python-regula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ith_instance(string, pattern, new_str, i = None, pattern_flags = 0):\n",
    "    # If i is None - replacing last occurrence\n",
    "    match_obj = re.finditer(r'{0}'.format(pattern), string, flags = pattern_flags)\n",
    "    matches = [item for item in match_obj]\n",
    "    if i == None:\n",
    "        i = len(matches)\n",
    "    if len(matches) == 0 or len(matches) < i:\n",
    "        return string\n",
    "    match = matches[i - 1]\n",
    "    match_start_index = match.start()\n",
    "    match_len = len(match.group())\n",
    "    return '{0}{1}{2}'.format(string[0:match_start_index], new_str, string[match_start_index + match_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_placeholders(chunk,placeholder='%%ADDRESS%%'):\n",
    "    fake=faker.Faker()\n",
    "    num_addr=count_occurences(chunk,placeholder)\n",
    "    addresses=[]\n",
    "    for i in range(num_addr):\n",
    "        fake_addr=fake.address()\n",
    "        chunk=replace_ith_instance(chunk,placeholder,fake_addr,1)\n",
    "        addresses.append(fake_addr)\n",
    "    return (chunk, tuple(addresses))\n",
    "\n",
    "def get_labeled_data(chunk_list):\n",
    "    labeled_data=[]\n",
    "    for chunk in chunk_list:\n",
    "        labeled_data.append(replace_placeholders(chunk))\n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_test=synthetic_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data=get_labeled_data(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "print(labeled_data[0])\n",
    "print(c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "https://towardsdatascience.com/addressnet-how-to-build-a-robust-street-address-parser-using-a-recurrent-neural-network-518d97b9aebd  \n",
    "https://www.tensorflow.org/tutorials/text/text_classification_rnn  \n",
    "https://keras.io/examples/lstm_seq2seq/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('87268 Gabrielle Roads\\nLake John, MT 60899',\n",
       " '13060 Sheila Stream Apt. 293\\nEast Dennis, NJ 03143',\n",
       " '184 Donald Rapids Apt. 621\\nSouth Kyle, AZ 62171')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_df=pd.DataFrame(labeled_data,columns=['Notice','Addresses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_df.to_csv('.\\labeled_data.txt',sep='|',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
