{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach  \n",
    "1. Get some random texts from eviction notices (10-30)\n",
    "2. Augument these random texts to 10000 using text augmentation techniques\n",
    "3. Randomly generate fake addresses and insert them into specific and random positions in the 10,000 texts\n",
    "4. At the same time assign the generated fake addresses as a label to that text\n",
    "5. Recurrent Neural networks to extract addresses\n",
    "6. User Address parser to extract and label address entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission\n",
    "- Report\n",
    "- Code Documentation\n",
    "- Functions input and Output\n",
    "- Usage\n",
    "- Limitations\n",
    "- size of input text, output text - pages, words, rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update  \n",
    "- Finshed trainng data creation in Fall 2019\n",
    "- Modelling part in Winter & Spring 2020 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks 3/6\n",
    "- Create Labelled Data - Assign flags to addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import usaddress\n",
    "import pytesseract\n",
    "import cv2\n",
    "import glob\n",
    "import faker\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import spacy\n",
    "from py_thesaurus import Thesaurus\n",
    "from nltk.corpus import wordnet \n",
    "import en_core_web_sm\n",
    "import re\n",
    "from nlp_aug import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "#### Parse Addresses  \n",
    "For International addresses: 'https://github.com/openvenues/libpostal'  \n",
    "For USA Addresses: https://github.com/datamade/usaddress\n",
    "#### OCR\n",
    "https://pypi.org/project/pytesseract/\n",
    "#### Supporting Libraries\n",
    "cv2, image/PIL.image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Augmentation Techniques  \n",
    "- Synonym Replacement (SR): Randomly replace n words in the sentences with their synonyms.\n",
    "- Random Insertion (RI): Insert random synonyms of words in a sentence, this is done n times.\n",
    "- Random Swap (RS): Two words in the sentences are randomly swapped, this is repeated n-times.\n",
    "- Random Deletion (RD): Random removal for each word in the sentence with a probability p.  \n",
    "The formula used to determine the number of sentences augmented is:  \n",
    "N = Alpha * Length of the sentence.  \n",
    "Alpha is the “augmentation parameter”, higher the alpha-more aggressive the “EDA”.  (Easy Data Augmentation)\n",
    "  \n",
    "Functions: https://www.kaggle.com/init927/nlp-data-augmentation\n",
    "Paper: https://arxiv.org/pdf/1901.11196.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checker\n",
    "https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "1. Creation of Synthetic Data\n",
    "        1. Gather Text\n",
    "        2. Convert Text into Documents\n",
    "        3. Add placeholders to the Text in decided and random locations\n",
    "            - %%ADDRESS%%, %%NAME%%, %%DATE%%, %%EMAIL%%, %%PHONE%%\n",
    "        4. Augment the Text along without touching the placeholders\n",
    "        5. Utility to replace the placeholders with corresponding randomly generated texts\n",
    "        6. Create a labeled dataset of (Documents, Addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake=faker.Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file_list=glob.glob(r\"synthetic-data\\*\")\n",
    "print(img_file_list)\n",
    "img = cv2.imread(r'synthetic-data\\image (3).jpg')\n",
    "print(pytesseract.image_to_string(img))\n",
    "# OR explicit beforehand converting\n",
    "#print(pytesseract.image_to_string(Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_extract(file_path=r\".\\synthetic-data\\extract.txt\"):\n",
    "    extract=open(file_path,'r',encoding='utf-8')\n",
    "    extracted=extract.read()\n",
    "    extract.close()\n",
    "    return extracted\n",
    "\n",
    "def fix_extract(extract, threshold=4):\n",
    "    sentences=nltk.sent_tokenize(extract)\n",
    "    filtered_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        if len(nltk.word_tokenize(sentence))>threshold:\n",
    "            filtered_sentences.append(sentence)\n",
    "    return \" \".join(filtered_sentences)\n",
    "        \n",
    "def chunk_text(large_chunk,num=10):\n",
    "    sentences=nltk.sent_tokenize(large_chunk)\n",
    "    size=len(sentences)\n",
    "    mini_chunks=[]\n",
    "    start=0\n",
    "    end=num\n",
    "    for i in range(size//num):\n",
    "        mini_chunks.append(\" \".join(sentences[start:end]))\n",
    "        start=start+num\n",
    "        end=end+num\n",
    "    return mini_chunks\n",
    "\n",
    "def read_samples(file_path=r\".\\synthetic-data\\text\\*\"):\n",
    "    file_list = glob.glob(file_path)\n",
    "    corpus = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path) as f_input:\n",
    "            corpus.append(f_input.read())\n",
    "    return corpus\n",
    "\n",
    "def insert_placeholder(string, index, placeholder,prob_newline=[0.3,0.3,0.3]):\n",
    "    replace_string=placeholder\n",
    "    if np.random.uniform(0,1)<prob_newline[0]:\n",
    "        replace_string=replace_string.replace('\\n','')\n",
    "    if np.random.uniform(0,1)<prob_newline[1]:\n",
    "        replace_string='\\n'+replace_string\n",
    "    else:\n",
    "        replace_string=' '+replace_string\n",
    "    if np.random.uniform(0,1)<prob_newline[2]:\n",
    "        replace_string=replace_string+'\\n'\n",
    "    else:\n",
    "        replace_string=replace_string+' '\n",
    "    return string[:index] + replace_string + string[index:]\n",
    "\n",
    "def randomly_assign_placeholder(mini_chunk,probabilities=[0.8,0.4,0.7], placeholder=r'%%ADDRESS%%'):\n",
    "    chunk_size=len(mini_chunk)\n",
    "    if np.random.uniform(0,1)<probabilities[0]:\n",
    "        index=random.randint(0,chunk_size//3)\n",
    "        index=index+mini_chunk[index:].find(' ')\n",
    "        mini_chunk=insert_placeholder(mini_chunk,index,placeholder)\n",
    "    chunk_size=len(mini_chunk)\n",
    "    if np.random.uniform(0,1)<probabilities[1]:\n",
    "        index=random.randint(chunk_size//3,2*chunk_size//3)\n",
    "        index=index+mini_chunk[index:].find(' ')\n",
    "        mini_chunk=insert_placeholder(mini_chunk,index,placeholder)\n",
    "    chunk_size=len(mini_chunk)\n",
    "    if np.random.uniform(0,1)<probabilities[0]:\n",
    "        index=random.randint(2*chunk_size//3,chunk_size)\n",
    "        index=index+mini_chunk[index:].rfind(' ')\n",
    "        mini_chunk=insert_placeholder(mini_chunk,index,placeholder)\n",
    "    return mini_chunk\n",
    "\n",
    "def randomly_assign_placeholder_to_list(mini_chunks,probabilities=[0.8,0.4,0.7],placeholder=r'%%ADDRESS%%'):\n",
    "    processed_chunks=[]\n",
    "    for chunk in mini_chunks:\n",
    "        new_chunk=randomly_assign_placeholder(chunk,probabilities,placeholder)\n",
    "        processed_chunks.append(new_chunk)\n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted=fix_extract(read_extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_chunks=chunk_text(extracted,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synalter_Noun_Verb(word,al,POS):\n",
    "    max_temp = -1\n",
    "    flag = 0\n",
    "    for i in a1:\n",
    "        try:\n",
    "            w1 = wordnet.synset(word+'.'+POS+'.01') \n",
    "            w2 = wordnet.synset(i+'.'+POS+'.01') # n denotes noun \n",
    "            if(max_temp<w1.wup_similarity(w2)):\n",
    "                max_temp=w1.wup_similarity(w2)\n",
    "                temp_name = i\n",
    "                flag =1\n",
    "        except:\n",
    "            f = 0\n",
    "            \n",
    "    if flag == 0:\n",
    "        max1 = -1.\n",
    "        nlp = en_core_web_sm.load()\n",
    "        for i in a1:\n",
    "            j=i.replace(' ', '')\n",
    "            tokens = nlp(u''+j)\n",
    "            token_main = nlp(u''+word_str)\n",
    "            for token1 in token_main:\n",
    "                if max1<float(token1.similarity(tokens)):\n",
    "                    max1 = token1.similarity(tokens)\n",
    "                    value = i\n",
    "        max1 = -1.\n",
    "        return value \n",
    "    else:\n",
    "        return temp_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonyms(chunks,percent=50):\n",
    "    output_chunks=[]\n",
    "    for chunk in chunks:\n",
    "        output_chunk = chunk\n",
    "        words = chunk.split()\n",
    "        counts = {}\n",
    "        for word in words:\n",
    "            if word not in counts:\n",
    "                counts[word] = 0\n",
    "            counts[word] += 1\n",
    "        one_word = []\n",
    "        for key, value in counts.items():\n",
    "            if value == 1 and key.isalpha() and len(key)>2:\n",
    "                one_word.append(key)\n",
    "        noun = []\n",
    "        verb = []\n",
    "        nlp = en_core_web_sm.load()\n",
    "        doc = nlp(u''+' '.join(one_word))\n",
    "        for token in doc:\n",
    "            if  token.pos_ == 'VERB':\n",
    "                verb.append(token.text)\n",
    "            if  token.pos_ == 'NOUN':\n",
    "                noun.append(token.text)\n",
    "        all_main =verb + noun\n",
    "        len_all = len(noun)+len(verb)\n",
    "        final_value = int(len_all * percent /100)\n",
    "        #random.seed(4)\n",
    "        temp = random.sample(range(0, len_all), final_value)\n",
    "        for i in temp:\n",
    "            try:\n",
    "                word_str = all_main[i]\n",
    "                w = Word(word_str)\n",
    "                a1= list(w.synonyms())\n",
    "                if i<len(verb):\n",
    "                    change_word=synalter_Noun_Verb(word_str,a1,'v')\n",
    "                    try:\n",
    "                        search_word = re.search(r'\\b('+word_str+r')\\b', output_chunk)\n",
    "                        Loc = search_word.start()\n",
    "                        output_chunk = output_chunk[:int(Loc)] + change_word + output_chunk[int(Loc) + len(word_str):]\n",
    "                    except:\n",
    "                        f=0\n",
    "\n",
    "                else:\n",
    "                    change_word=synalter_Noun_Verb(word_str,a1,'n')\n",
    "                    try:\n",
    "                        search_word = re.search(r'\\b('+word_str+r')\\b', output_chunk)\n",
    "                        Loc = search_word.start()\n",
    "                        output_chunk = output_chunk[:int(Loc)] + change_word + output_chunk[int(Loc) + len(word_str):]\n",
    "                    except:\n",
    "                        f=0\n",
    "            except:\n",
    "                f=0\n",
    "        output_chunks.append(output_chunk)\n",
    "    return output_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Augment Functions built upon nlp_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_chunk(mini_chunk, probability=0.5):\n",
    "    sentences=nltk.sent_tokenize(mini_chunk)\n",
    "    new_sentences=[]\n",
    "    for sentence in sentences:      \n",
    "        if random.uniform(0,1)<=probability:\n",
    "            new_sentence=eda_4(sentences[0], num_aug=1)[0]+'.'\n",
    "            new_sentences.append(new_sentence)\n",
    "        else:\n",
    "            new_sentences.append(sentence)\n",
    "    return \" \".join(new_sentences)\n",
    "\n",
    "def augment_list_of_chunks(mini_chunks,probability=0.5, num_aug=100):\n",
    "    augmented_chunks=[]\n",
    "    num=0\n",
    "    for i in range(num_aug):\n",
    "        for chunk in mini_chunks:\n",
    "            try:\n",
    "                num=num+1\n",
    "                augmented_chunks.append(augment_chunk(chunk,probability)) \n",
    "            except:\n",
    "                print(\"Error in Chunk List: Chunk#\" +str(num-1))\n",
    "                return None\n",
    "    augmented_chunks.extend(mini_chunks)\n",
    "    return augmented_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment chunks derived from the text extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_chunks=augment_list_of_chunks(mini_chunks, num_aug=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Address placeholders to the augmented chunks derived from the text extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks=randomly_assign_placeholder_to_list(ag_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Samples and Augment Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_notices=read_samples()\n",
    "aug_sample_notices=augment_list_of_chunks(sample_notices, num_aug=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data=processed_chunks+aug_sample_notices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace %%ADDRESS%%  placeholders with random fake addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurences(source_string, substring):\n",
    "    return re.subn(substring, '', source_string)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/27589325/how-to-find-and-replace-nth-occurrence-of-word-in-a-sentence-using-python-regula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ith_instance(string, pattern, new_str, i = None, pattern_flags = 0):\n",
    "    # If i is None - replacing last occurrence\n",
    "    match_obj = re.finditer(r'{0}'.format(pattern), string, flags = pattern_flags)\n",
    "    matches = [item for item in match_obj]\n",
    "    if i == None:\n",
    "        i = len(matches)\n",
    "    if len(matches) == 0 or len(matches) < i:\n",
    "        return string\n",
    "    match = matches[i - 1]\n",
    "    match_start_index = match.start()\n",
    "    match_len = len(match.group())\n",
    "    return '{0}{1}{2}'.format(string[0:match_start_index], new_str, string[match_start_index + match_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_placeholders(chunk,placeholder='%%ADDRESS%%', newline_flag=False):\n",
    "    fake=faker.Faker()\n",
    "    num_addr=count_occurences(chunk,placeholder)\n",
    "    addresses=[]\n",
    "    for i in range(num_addr):\n",
    "        fake_addr=fake.address()\n",
    "        chunk=replace_ith_instance(chunk,placeholder,fake_addr,1)\n",
    "        addresses.append(fake_addr.replace('\\n',' '))\n",
    "    if newline_flag==False:\n",
    "        chunk=chunk.replace('\\n',' ')\n",
    "    return (chunk, tuple(addresses))\n",
    "\n",
    "def get_labeled_data(chunk_list):\n",
    "    labeled_data=[]\n",
    "    for chunk in chunk_list:\n",
    "        labeled_data.append(replace_placeholders(chunk))\n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data=get_labeled_data(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "print(labeled_data[5][0])\n",
    "print(synthetic_data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "(WIP and not good documentation)\n",
    "\n",
    "https://towardsdatascience.com/addressnet-how-to-build-a-robust-street-address-parser-using-a-recurrent-neural-network-518d97b9aebd  \n",
    "https://www.tensorflow.org/tutorials/text/text_classification_rnn  \n",
    "https://keras.io/examples/lstm_seq2seq/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_df=pd.DataFrame(labeled_data,columns=['Notice','Addresses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeled_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-67a7cf34c69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.\\labeled_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'labeled_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "labeled_data_df.to_csv('.\\labeled_data.txt',sep='|',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_df=pd.read_csv('labeled_data.txt',sep='|')\n",
    "labeled_data_df[['Addresses']]=pd.DataFrame(labeled_data_df['Addresses'].apply(lambda t:ast.literal_eval(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PSC 8763, Box 5203 APO AE 49080',)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data_df.iloc[0]['Addresses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Eviction Notice, also known as a Notice to Quit, is a document sent by a Landlord to a Tenant to inform them of a violation or termination of the lease agreement and to start the process of removing a Tenant from the property. an eviction also to quit PSC 8763, Box 5203 APO AE 49080 a document sent by a landlord to a tenant to inform them of a violation or termination of the lease agreement to start the process removing a tenant from the property. However, this eviction process generally begins with the Landlord providing the Tenant with a written Eviction Notice. an eviction notice also to as a notice to quit a a document them by a the to a tenant known inform sent of a violation or termination of landlord lease agreement and to start the process of removing is tenant from the property. The Eviction Notice serves to make the Tenant aware that they have not complied with the terms of the lease or are otherwise subject to being evicted and gives the Tenant a deadline by which they must either correct the issue or make arrangements to leave the property. an eviction termination also known tenant a notice to quit is a from sent by a landlord to a as to inform them of a violation or notice of the lease agreement and to start the process of removing the tenant document a property. an eviction notice known a notice to quit is a document sent by a landlord to inform them of violation or termination of lease agreement and to start the process of removing a tenant from the property. This Notice includes important facts about the property and lease agreement, including the parties to the lease, address of the rental property, the date the Tenant entered into the lease agreement, a description of the reason why the Tenant is being evicted, and, if applicable, instructions about how the Tenant can cure the lease violation and the deadline by which they must take action to cure their violation and come in compliance with the lease. ampere an eviction notice also known as a notice to quit is a document sent by amp a landlord to a tenant to renter ampere amp inform them of ampere a violation or renter termination of the lease agreement and to start the amp process of removing a tenant from the property. Generally, an Eviction Notice is sent by certified mail or delivered in person, so as to create a record that the letter was sent and received by the Tenant.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data_df.iloc[0]['Notice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_df[['Notice']]=pd.DataFrame(labeled_data_df['Notice'].apply(lambda t:t.replace('\\n',' ')))\n",
    "labeled_data_df[['Notice']]=pd.DataFrame(labeled_data_df['Notice'].apply(lambda t:' '.join(t.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Eviction Notice, also known as a Notice to Quit, is a document sent by a Landlord to a Tenant to inform them of a violation or termination of the lease agreement and to start the process of removing a Tenant from the property. an eviction also to quit PSC 8763, Box 5203 APO AE 49080 a document sent by a landlord to a tenant to inform them of a violation or termination of the lease agreement to start the process removing a tenant from the property. However, this eviction process generally begins with the Landlord providing the Tenant with a written Eviction Notice. an eviction notice also to as a notice to quit a a document them by a the to a tenant known inform sent of a violation or termination of landlord lease agreement and to start the process of removing is tenant from the property. The Eviction Notice serves to make the Tenant aware that they have not complied with the terms of the lease or are otherwise subject to being evicted and gives the Tenant a deadline by which they must either correct the issue or make arrangements to leave the property. an eviction termination also known tenant a notice to quit is a from sent by a landlord to a as to inform them of a violation or notice of the lease agreement and to start the process of removing the tenant document a property. an eviction notice known a notice to quit is a document sent by a landlord to inform them of violation or termination of lease agreement and to start the process of removing a tenant from the property. This Notice includes important facts about the property and lease agreement, including the parties to the lease, address of the rental property, the date the Tenant entered into the lease agreement, a description of the reason why the Tenant is being evicted, and, if applicable, instructions about how the Tenant can cure the lease violation and the deadline by which they must take action to cure their violation and come in compliance with the lease. ampere an eviction notice also known as a notice to quit is a document sent by amp a landlord to a tenant to renter ampere amp inform them of ampere a violation or renter termination of the lease agreement and to start the amp process of removing a tenant from the property. Generally, an Eviction Notice is sent by certified mail or delivered in person, so as to create a record that the letter was sent and received by the Tenant.\n"
     ]
    }
   ],
   "source": [
    "print(labeled_data_df['Notice'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Enegineering\n",
    "- I am trying out 1-gram text sequences to start with\n",
    "- It is easy to encode text-sequences into 1-gram\n",
    "- There are limited number of features and the number of feature remains constant\n",
    "- I am interested in alpha-numeric characters and spcecial characters like ',', '.', ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add hash and dash sign as well\n",
    "features='1234567890,.abcdefghijklmnopqrstuvwxyz '\n",
    "feature_dict=defaultdict(int)\n",
    "count=0\n",
    "for f in features:\n",
    "    count+=1\n",
    "    feature_dict[f] += count # increment element's value by 1\n",
    "\n",
    "inv_feature_dict = {v: k for k, v in feature_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(text, feature_dict=feature_dict):\n",
    "    code=[]\n",
    "    text=text.lower()\n",
    "    for charac in text:\n",
    "        code.append(feature_dict[charac])\n",
    "    code=np.array(code)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_features(seq, mapping=inv_feature_dict):\n",
    "    seq_d=[]\n",
    "    for num in seq:\n",
    "        n=int(np.round(num[0]))\n",
    "        seq_d.append(mapping[n])\n",
    "    return ''.join(seq_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erroneous Archived Function\n",
    "def mask_notices(notice, addresses):\n",
    "    len_addr=len(addresses)\n",
    "    indices=[]\n",
    "    if len_addr==0:\n",
    "        return '_'*len(notice)\n",
    "    else:\n",
    "        start_index=0\n",
    "        ret_string=''\n",
    "        for addr in addresses:\n",
    "            addr_ind=notice[start_index:].index(addr)\n",
    "            indices.append(addr_ind)\n",
    "            start_index=len(addr)+addr_ind\n",
    "        for i in range(len_addr):\n",
    "            ret_string=ret_string+'_'*indices[i]+addresses[i]\n",
    "        delta=len(notice)-len(ret_string)\n",
    "        if delta<0:\n",
    "            print('Issue with the data. Output Sequence > Input')\n",
    "        return ret_string+'_'*delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_notices(notice, addresses):\n",
    "    len_addr=len(addresses)\n",
    "    len_not=len(notice)\n",
    "    indices=[]\n",
    "    if len_addr==0:\n",
    "        return '_'*len_not\n",
    "    else:\n",
    "        bool_vec=np.zeros(len_not, dtype=np.bool)\n",
    "        for addr in addresses:\n",
    "            start_ind=notice.index(addr)\n",
    "            end_ind=start_ind+len(addr)\n",
    "            bool_vec[start_ind:end_ind]=True\n",
    "        masked_notice=''\n",
    "        for c,b in zip(notice, bool_vec):\n",
    "            if b:\n",
    "                masked_notice=masked_notice+c\n",
    "            else:\n",
    "                masked_notice=masked_notice+'_'\n",
    "        return masked_notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH=max(labeled_data_df['Notice'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8038"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH=8100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(text_seq,max_len=MAX_SEQ_LENGTH):\n",
    "    padding=MAX_SEQ_LENGTH-len(text_seq)\n",
    "    return text_seq+padding*'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getXY(labeled_data_df=labeled_data_df):\n",
    "subset=labeled_data_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=subset['Notice'].apply(lambda l: pad_text(l) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=subset['Notice'] \\\n",
    "      .apply(lambda l: pad_text(l)) \\\n",
    "      .apply(lambda l: encode_features(l)) \\\n",
    "      .apply(lambda l: np.array(list(map(lambda m:np.float(m), l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t=subset \\\n",
    "      .apply(lambda l: mask_notices(l['Notice'],l['Addresses']), axis=1) \\\n",
    "      .apply(lambda l: pad_text(str(l))) \\\n",
    "      .apply(lambda l: encode_features(str(l))) \\\n",
    "      .apply(lambda l: np.array(list(map(lambda m:np.float(m), l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0. 28. 31. 15. 39.  8.  7.  6.  3. 11. 39. 14. 27. 36. 39.  5.  2. 10.\n",
      "  3. 39. 13. 28. 27. 39. 13. 17. 39.  4.  9. 10.  8. 10.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_t[0][200:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19., 39., 13., 39., 32., 17., 26., 13., 26., 32., 39., 18., 30.,\n",
       "       27., 25., 39., 32., 20., 17., 39., 28., 30., 27., 28., 17., 30.,\n",
       "       32., 37., 12., 39., 13., 26., 39., 17., 34., 21., 15., 32., 21.,\n",
       "       27., 26., 39., 13., 24., 31., 27., 39., 32., 27., 39., 29., 33.,\n",
       "       21., 32., 39., 28., 31., 15., 39.,  8.,  7.,  6.,  3., 11., 39.,\n",
       "       14., 27., 36., 39.,  5.,  2., 10.,  3., 39., 13., 28., 27., 39.,\n",
       "       13., 17., 39.,  4.,  9., 10.,  8., 10., 39., 13., 39., 16., 27.,\n",
       "       15., 33., 25., 17., 26., 32., 39., 31., 17.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t[0][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d5adb721fa51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpad_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencode_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-d5adb721fa51>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(l)\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpad_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencode_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-d5adb721fa51>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(m)\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpad_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencode_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m       \u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_raw=labeled_data_df['Notice'] \\\n",
    "      .apply(lambda l: pad_text(l)) \\\n",
    "      .apply(lambda l: encode_features(l)) \\\n",
    "      .apply(lambda l: np.array(list(map(lambda m:np.float(m), l))))\n",
    "               \n",
    "Y_raw=labeled_data_df \\\n",
    "      .apply(lambda l: mask_notices(l['Notice'],l['Addresses']), axis=1) \\\n",
    "      .apply(lambda l: pad_text(str(l))) \\\n",
    "      .apply(lambda l: encode_features(str(l))) \\\n",
    "      .apply(lambda l: np.array(list(map(lambda m:np.float(m), l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw=np.concatenate(X_raw)#.reshape(1443,8038)\n",
    "Y_raw=np.concatenate(Y_raw)#.reshape(1443,8038)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [13.0, 26.0, 39.0, 17.0, 34.0, 21.0, 15.0, 32....\n",
       "1    [32.0, 20.0, 21.0, 31.0, 39.0, 30.0, 17.0, 15....\n",
       "2    [13.0, 32.0, 39.0, 32.0, 20.0, 17.0, 39.0, 17....\n",
       "3    [15.0, 33.0, 30.0, 17.0, 39.0, 27.0, 30.0, 39....\n",
       "4    [21.0, 18.0, 39.0, 37.0, 27.0, 33.0, 39.0, 31....\n",
       "5    [18.0, 27.0, 30.0, 39.0, 25.0, 27.0, 30.0, 17....\n",
       "6    [32.0, 20.0, 17.0, 39.0, 28.0, 30.0, 21.0, 15....\n",
       "7    [25.0, 27.0, 31.0, 32.0, 39.0, 27.0, 26.0, 24....\n",
       "8    [25.0, 21.0, 15.0, 30.0, 27.0, 31.0, 27.0, 18....\n",
       "9    [18.0, 27.0, 30.0, 39.0, 28.0, 33.0, 30.0, 28....\n",
       "Name: Notice, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 1 2 3 4 5 \n",
    "1 2 3 4 5 6\n",
    "2 3 4 5 6 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalize(X,y,lookback):\n",
    "    Xt=[]\n",
    "    yt=[]\n",
    "    for i in range(len(X)-lookback+1):\n",
    "        Xt.append(X[i:i+lookback])\n",
    "        yt.append(y[i:i+lookback])\n",
    "    return Xt,yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporalize(X, y, lookback):\n",
    "    output_X = []\n",
    "    output_y = []\n",
    "    for i in range(len(X)-lookback-1):\n",
    "        t = []\n",
    "        u = []\n",
    "        for j in range(1,lookback+1):\n",
    "            t.append(np.array(X[[(i+j+1)]]))\n",
    "            u.append(np.array(y[[(i+j+1)]]))\n",
    "        output_X.append(t)\n",
    "        output_y.append(u)\n",
    "        #output_y.append(y[i+lookback+1])\n",
    "    return output_X, output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 30\n",
    "n_features = 1\n",
    "\n",
    "X, Y = temporalize(X = X_raw, y = Y_raw, lookback = timesteps)\n",
    "\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0], timesteps, n_features)\n",
    "\n",
    "Y = np.array(Y)\n",
    "Y = Y.reshape(Y.shape[0], timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileX = open('tempo_X.pickle', 'ab')\n",
    "pickle.dump(X,fileX)\n",
    "fileY = open('tempo_Y.pickle', 'ab')\n",
    "pickle.dump(Y,fileY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileX.close()\n",
    "fileY.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_shapes(Sequences, Targets): # can make yours to take inputs; this'll use local variable values\n",
    "    print(\"Expected: (num_samples, timesteps, channels)\")\n",
    "    print(\"Sequences: {}\".format(Sequences.shape))\n",
    "    print(\"Targets:   {}\".format(Targets.shape))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11598805"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence to sequence LSTMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 30, 256)           264192    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 30, 144)           230976    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 144)           0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 30, 144)           166464    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30, 144)           0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 128)               139776    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 30, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 30, 108)           102384    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 30, 108)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 30, 108)           93744     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 30, 108)           0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 30, 64)            44288     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 1)             65        \n",
      "=================================================================\n",
      "Total params: 1,173,473\n",
      "Trainable params: 1,173,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, activation='relu', input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model.add(LSTM(144, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(144, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "model.add(RepeatVector(timesteps))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(108, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(108, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11598805 samples\n",
      "Epoch 1/5\n",
      "11598805/11598805 [==============================] - 15458s 1ms/sample - loss: 783.7262\n",
      "Epoch 2/5\n",
      "11598805/11598805 [==============================] - 15713s 1ms/sample - loss: 8.5163\n",
      "Epoch 3/5\n",
      "11598805/11598805 [==============================] - 15567s 1ms/sample - loss: 7.1938\n",
      "Epoch 4/5\n",
      "11598805/11598805 [==============================] - 15244s 1ms/sample - loss: 5.7712\n",
      "Epoch 5/5\n",
      "11598805/11598805 [==============================] - 15380s 1ms/sample - loss: 5.5536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb601fdcf8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=5, batch_size=2500, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: mymodel.model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('mymodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTM_TextExtract/assets\n"
     ]
    }
   ],
   "source": [
    "save_model(model,'LSTM_TextExtract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(timesteps,n_features))\n",
    "inp_1=LSTM(128, activation='relu', input_shape=(timesteps,n_features), return_sequences=True)(inp)\n",
    "inp_dp_1=Dropout(0.2)(inp_1)\n",
    "inp_2=LSTM(3, activation='relu', return_sequences=False)(inp_dp_1)\n",
    "rep=RepeatVector(timesteps)(inp_2)\n",
    "op_2=LSTM(3, activation='relu', return_sequences=True)(rep)\n",
    "op_dp_1=Dropout(0.2)(op_2)\n",
    "op_1=LSTM(128, activation='relu', return_sequences=True)(op_dp_1)\n",
    "op=TimeDistributed(Dense(n_features))(op_1)\n",
    "\n",
    "autoencoder=Model(inp,op)\n",
    "encoder=Model(inp,rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adamax', loss='mse')\n",
    "autoencoder.fit(X, X, epochs=100, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 30\n",
    "n_features = 1\n",
    "X_tr=np.concatenate(X_t)\n",
    "X_test, y = temporalize(X = X_tr, y = X_tr, lookback = timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape(X_test.shape[0], timesteps, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 62s 4ms/sample\n"
     ]
    }
   ],
   "source": [
    "yhat = loaded_model.predict(X_test[5000:20000], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset['Notice'][0].index('PSC 8763')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
